# 1) Data Cleaning 

rm(list = ls(all.names = TRUE))
gc()
cat("\014")  

april_to_june_2019 <- read.csv("/Users/SakinaMNazarali/Downloads/2019_Yellow_Taxi_Trip_Data_April_to_June.csv")
april_to_june_2020 <- read.csv("/Users/SakinaMNazarali/Downloads/2020_Yellow_Taxi_Trip_Data_April_to_June.csv")
taxi_zones <- read.csv("/Users/SakinaMNazarali/Downloads/taxi+_zone_lookup.csv")
nrow(april_to_june_2019) # 21772272
nrow(april_to_june_2020) # 1110871 

set.seed(12345)
install.packages("dplyr")
library(dplyr)
require(data.table)
require(pastecs)

# The 2019 dataset has over 21 million records. Randomly select 2 million records.
atj_2019 <- sample_n(april_to_june_2019,2000000)

nrow(atj_2019) # 2000000
atj_2020 <- april_to_june_2020
str(atj_2019)
str(atj_2020)

# The dates are currently incharacters format + AM/PM. In order to be able to perform an analysis on
# them, they need to be changed to POSIXct + military timing
str(atj_2019$tpep_pickup_datetime) #  chr 
str(atj_2019$tpep_dropoff_datetime) #  chr 
str(atj_2020$tpep_pickup_datetime) #  chr 
str(atj_2020$tpep_dropoff_datetime) #  chr 
install.packages("lubridate")
install.packages("tidyverse")
library(lubridate)
atj_2019$tpep_pickup_datetime = parse_date_time(atj_2019$tpep_pickup_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2019$tpep_pickup_datetime) # POSIXct
atj_2019$tpep_dropoff_datetime = parse_date_time(atj_2019$tpep_dropoff_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2019$tpep_dropoff_datetime) # POSIXct
atj_2020$tpep_pickup_datetime = parse_date_time(atj_2020$tpep_pickup_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2020$tpep_pickup_datetime) # POSIXct
atj_2020$tpep_dropoff_datetime = parse_date_time(atj_2020$tpep_dropoff_datetime, c('Ymd HM', 'mdy IMS p'))
str(atj_2020$tpep_dropoff_datetime) # POSIXct
# A time that was previously written as "06/18/2019 03:38:34 PM" is now written as "2019-06-18 15:38:34"

# Ensure that the data we have is within the timeframe we are interested in: 
nrow(atj_2019) # 2000000
atj_2019 <- atj_2019[atj_2019$tpep_pickup_datetime >= "2019-04-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-06-30 11:59:59",]
nrow(atj_2019) # 1998403
nrow(atj_2020) # 1110871
atj_2020 <- atj_2020[atj_2020$tpep_pickup_datetime >= "2020-04-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-06-30 11:59:59",]
nrow(atj_2020) # 1109997
# Therefore, data records that were beyond our timeframe have now been ommitted. Note the difference in number of
# rows before filtering for the times.

# Check if the randomization was fair (only performed on 2019 data, given that the dataset was huge) and that each month has an approximately 
# equal number of records, subset the data by month:
april_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-04-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-04-30 11:59:59",]
may_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-05-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-05-31 11:59:59",]
june_2019 = atj_2019[atj_2019$tpep_pickup_datetime >= "2019-06-01 00:00:00" & atj_2019$tpep_pickup_datetime <= "2019-06-30 11:59:59",]
nrow(april_2019) # 673954
nrow(may_2019) # 687271
nrow(june_2019) # 618196

# To perform a similar check on 2020 data (even though the dataset was not randomly sampled), subset the data by months:
april_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-04-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-04-30 11:59:59",]
may_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-05-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-05-31 11:59:59",]
june_2020 = atj_2020[atj_2020$tpep_pickup_datetime >= "2020-06-01 00:00:00" & atj_2020$tpep_pickup_datetime <= "2020-06-30 11:59:59",]
nrow(april_2020) # 235508
nrow(may_2020) # 345519
nrow(june_2020) # 522945

# Drop unnecessary columns
ncol(atj_2019) # 18
atj_2019 = select(atj_2019, -c(store_and_fwd_flag,improvement_surcharge,congestion_surcharge, mta_tax))
ncol(atj_2019) # 14
ncol(atj_2020) # 18 
atj_2020 = select(atj_2020, -c(store_and_fwd_flag,improvement_surcharge,congestion_surcharge, mta_tax))
ncol(atj_2020) # 14

# For individuals who made a cash payment, the tip fare is recorded as 0, even though they may have made a tip. 
# The total fare of the trip is present in the dataset, but this would mean that tip/total fare = 0 for such rides
# which may skew the data.

# The number of trips that were performed using cash payment in the 2019 and 2020 dataset: 
sum(atj_2019$payment_type=='2') # 544475
sum(atj_2019$payment_type=='2')/nrow(atj_2019) # 0.2724586 = 27.2% of all trips in 2019 were paid via cash
sum(atj_2020$payment_type=='2', na.rm = TRUE) # 353495
sum(atj_2020$payment_type=='2', na.rm = TRUE)/nrow(atj_2020) # 0.3184648 = 31.8% of all trips in 2020 were paid via cash

# Drop all rows that were performed using a cash payment. payment_type == 2. 
nrow(atj_2019) # 1998377
atj_2019 <- subset(atj_2019, payment_type!="2")
nrow(atj_2019) # 1453902
nrow(atj_2020) # 1109997
atj_2020 <- subset(atj_2020, payment_type!="2")
nrow(atj_2020) # 629492

# Merge the location file to associate pick up and drop off locations with names over numbers.
# Whilst this does not necessarily make a difference in the analysis, it will in the interpretation. 
merged2019 <- merge(atj_2019,taxi_zones,by.x = c("PULocationID"),by.y = c("LocationID"))
names(merged2019)[names(merged2019) == 'Zone'] <- 'PUZone'
names(merged2019)[names(merged2019) == 'Borough'] <- 'PUBorough'
names(merged2019)[names(merged2019) == 'service_zone'] <- 'PUservice_zone'
merged2019 <- merge(merged2019,taxi_zones,by.x = c("DOLocationID"),by.y = c("LocationID"))
names(merged2019)[names(merged2019) == 'Zone'] <- 'DOZone'
names(merged2019)[names(merged2019) == 'Borough'] <- 'DOBorough'
names(merged2019)[names(merged2019) == 'service_zone'] <- 'DOservice_zone'
colnames(merged2019)

merged2020 <- merge(atj_2020,taxi_zones,by.x = c("PULocationID"),by.y = c("LocationID"))
names(merged2020)[names(merged2020) == 'Zone'] <- 'PUZone'
names(merged2020)[names(merged2020) == 'Borough'] <- 'PUBorough'
names(merged2020)[names(merged2020) == 'service_zone'] <- 'PUservice_zone'
merged2020 <- merge(merged2020,taxi_zones,by.x = c("DOLocationID"),by.y = c("LocationID"))
names(merged2020)[names(merged2020) == 'Zone'] <- 'DOZone'
names(merged2020)[names(merged2020) == 'Borough'] <- 'DOBorough'
names(merged2020)[names(merged2020) == 'service_zone'] <- 'DOservice_zone'
colnames(merged2020)

# Checking if any columns have NA Values
names(merged2019)[sapply(merged2019, anyNA)] # "PUZone" "DOZone"
names(merged2020)[sapply(merged2020, anyNA)] # "PUZone" "DOZone"
# As mentioned earlier, the zone information (that were merged later on) are not of use in the analysis,
# but will come handy in the interpretation. Therefore, these data points will not need to be replaced or omitted.
# NAs are associated with Pick up and Drop Off locations 264 and 265. 

# Add a column which calculates the tip per trip as a fraction of the total fare
merged2019$tipdivtotal <- merged2019$tip_amount/merged2019$total_amount
merged2020$tipdivtotal <- merged2020$tip_amount/merged2020$total_amount
head(merged2019)
head(merged2020)

# Add a column that calculates the trip time (DOtime - PUtime) in seconds
merged2019$trip_time <- merged2019$tpep_dropoff_datetime - merged2019$tpep_pickup_datetime
head(merged2019$trip_time)
merged2020$trip_time <- merged2020$tpep_dropoff_datetime - merged2020$tpep_pickup_datetime
head(merged2020$trip_time)

# Final check 
names(merged2019) # check titles for each of the columns
head(merged2019,5) # check the first five rows for each column
str(merged2019) # check for the data types of each column (integer, num)
class(merged2019) # check for the data class of the dataframe
summary(merged2019) # perform quick overview of data
dim(merged2019) # check for the number of columns by rows

names(merged2020)
head(merged2020,5)
str(merged2020)
class(merged2020)
summary(merged2020)
dim(merged2020)

# 2) Question 1 - OLS - # What variables impact tipping behavior

# HOW TO CHECK FOR NORMALITY
# SUMMARY STATS, SUMMARY GRAPHS
# BASIC AND GRAPHICAL ANALYSIS 
